{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNk5nT6fJGPI72Zh+lPbMkD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vedant-Mahadik/DesignOptimizationFall2022/blob/main/Project01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "UeqBz3GItJ2T"
      },
      "outputs": [],
      "source": [
        "# overhead\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.nn import utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# environment parameters\n",
        "\n",
        "FRAME_TIME = 0.1  # time interval\n",
        "GRAVITY_ACCEL = 9.81  # gravity constant\n",
        "BOOST_ACCEL = 0.18  # thrust constant\n",
        "ROT_ACCEL = 0.10  # angular acceleration\n",
        "DRAG_ACCEL = 0.05  # acceleration due to drag\n",
        "\n",
        "\n",
        "\n",
        "# # the following parameters are not being used in the sample code\n",
        "# PLATFORM_WIDTH = 0.25  # landing platform width\n",
        "# PLATFORM_HEIGHT = 0.06  # landing platform height\n",
        "# ROTATION_ACCEL = 20  # rotation constant"
      ],
      "metadata": {
        "id": "QhK8J0atk1Me"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0LgAMhzHf4lW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# define system dynamics\n",
        "# Notes:\n",
        "# 0. You only need to modify the \"forward\" function\n",
        "# 1. All variables in \"forward\" need to be PyTorch tensors.\n",
        "# 2. All math operations in \"forward\" has to be differentiable, e.g., default PyTorch functions.\n",
        "# 3. Do not use inplace operations, e.g., x += 1. Please see the following section for an example that does not work.\n",
        "\n",
        "class Dynamics(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Dynamics, self).__init__()\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(state, action):\n",
        "      \n",
        "        # Apply gravity\n",
        "        # Note: Here gravity is used to change velocity which is the second element of the state vector\n",
        "        # Normally, we would do x[1] = x[1] + gravity * delta_time\n",
        "        # but this is not allowed in PyTorch since it overwrites one variable (x[1]) that is\n",
        "        # part of the computational graph to be differentiated.\n",
        "        # Therefore, I define a tensor dx = [0., gravity * delta_time], and do x = x + dx.\n",
        "        # This is allowed...\n",
        "        delta_state_gravity = t.tensor([0., GRAVITY_ACCEL * FRAME_TIME, 0., 0.])\n",
        "\n",
        "\n",
        "        # Thrust\n",
        "        # Note: Same reason as above. \n",
        "        # Note: Action provided by controller to determine thrust or no thrust.\n",
        "        delta_state = BOOST_ACCEL * FRAME_TIME * t.tensor([0., cos(state[2]), 0., 0.]) * action[0] #delta state y\n",
        "\n",
        "        temp = delta_state\n",
        "\n",
        "        #Updating drag\n",
        "        DRAG_DECEL = 0.0000243 * temp* temp\n",
        "        \n",
        "        NET_ACCEL = BOOST_ACCEL - DRAG_DECEL\n",
        "\n",
        "        #Re-calculating new velocity\n",
        "        delta_state = NET_ACCEL * FRAME_TIME * t.tensor([0., -1., 0., 0.]) * action[0] #delta state y\n",
        "\n",
        "        \n",
        "\n",
        "        # Drag\n",
        "        # Acts in opposite direction of velocity\n",
        "        #delta_state_drag = DRAG_ACCEL * FRAME_TIME * t.tensor([0., -1., 0., 0.])\n",
        "\n",
        "\n",
        "\n",
        "        # Rotational thrust\n",
        "        delta_state_rot = ROT_ACCEL * FRAME_TIME * t.tensor([0., 0., 0., 1.]) * action[1]\n",
        "\n",
        "        # Update velocity\n",
        "        state = state + delta_state + delta_state_gravity  + delta_state_rot #+ delta_state_drag\n",
        "        #state = state + delta_state + delta_state_gravity + delta_rotation_ang + delta_ang_accel\n",
        "\n",
        "        # Update state\n",
        "        # Note: Same as above. Use operators on matrices/tensors as much as possible.\n",
        "        # Do not use element-wise operators as they are considered inplace.\n",
        "        step_mat = t.tensor([[1., FRAME_TIME, 0., 0.],\n",
        "                            [0., 1., 0., 0.],\n",
        "                            [0., 0., 1., FRAME_TIME],\n",
        "                            [0., 0., 0., 1.]])\n",
        "        state = t.matmul(step_mat, state)\n",
        "\n",
        "        return state\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a deterministic controller\n",
        "# Note:\n",
        "# 0. You only need to change the network architecture in \"__init__\"\n",
        "# 1. nn.Sigmoid outputs values from 0 to 1, nn.Tanh from -1 to 1\n",
        "# 2. You have all the freedom to make the network wider (by increasing \"dim_hidden\") or\n",
        "# deeper (by adding more lines to nn.Sequential)\n",
        "# 3. Always start with something simple\n",
        "\n",
        "class Controller(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
        "        \"\"\"\n",
        "        dim_input: # of system states\n",
        "        dim_output: # of actions\n",
        "        dim_hidden: up to you\n",
        "        \"\"\"\n",
        "        super(Controller, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(dim_input, dim_hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(dim_hidden, dim_output),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    # Forward updates action, which is passed to dynamics as control for next step\n",
        "    def forward(self, state):\n",
        "        action = (self.network(state))\n",
        "        return action"
      ],
      "metadata": {
        "id": "lMylWhoEu-Uk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the simulator that rolls out x(1), x(2), ..., x(T)\n",
        "# Note:\n",
        "# 0. Need to change \"initialize_state\" to optimize the controller over a distribution of\n",
        "# initial states\n",
        "# 1. self.action_trajectory and self.state_trajectory stores the action and state\n",
        "# trajectories along time\n",
        "\n",
        "class Simulation(nn.Module):\n",
        "\n",
        "    def __init__(self, controller, dynamics, T):\n",
        "        super(Simulation, self).__init__()\n",
        "        self.state = self.initialize_state()\n",
        "        self.controller = controller\n",
        "        self.dynamics = dynamics\n",
        "        self.T = T\n",
        "        self.action_trajectory = []\n",
        "        self.state_trajectory = []\n",
        "\n",
        "    def forward(self, state):\n",
        "        self.action_trajectory = []\n",
        "        self.state_trajectory = []\n",
        "        for _ in range(T):\n",
        "            action = self.controller.forward(state)\n",
        "            state = self.dynamics.forward(state, action)\n",
        "            self.action_trajectory.append(action)\n",
        "            self.state_trajectory.append(state)\n",
        "        return self.error(state)\n",
        "\n",
        "    @staticmethod\n",
        "    def initialize_state():\n",
        "        state = [1., 0., -0.5, 0.]  # TODO: need batch of initial states\n",
        "        return t.tensor(state, requires_grad=False).float()  # TODO: Why does requires_grad=False?\n",
        "\n",
        "    def error(self, state):\n",
        "        return state[0]**2 + state[1]**2 + state[2]**2 + state[3]**2"
      ],
      "metadata": {
        "id": "uw7-y9dFvFWb"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the optimizer\n",
        "# Note:\n",
        "# 0. LBFGS is a good choice if you don't have a large batch size (i.e., a lot of initial states to consider\n",
        "# simultaneously)\n",
        "# 1. You can also try SGD and other momentum-based methods implemented in PyTorch\n",
        "# 2. You will need to customize \"visualize\"\n",
        "# 3. loss.backward is where the gradient is calculated (d_loss/d_variables)\n",
        "# 4. self.optimizer.step(closure) is where gradient descent is done\n",
        "\n",
        "class Optimize:\n",
        "    def __init__(self, simulation):\n",
        "        self.simulation = simulation\n",
        "        self.parameters = simulation.controller.parameters()\n",
        "        self.optimizer = optim.LBFGS(self.parameters, lr=0.01)\n",
        "        self.loss = []\n",
        "\n",
        "    def step(self):\n",
        "        def closure():\n",
        "            loss = self.simulation(self.simulation.state)\n",
        "            self.optimizer.zero_grad()  # TODO: Why is this necessary?\n",
        "            loss.backward()\n",
        "            return loss\n",
        "\n",
        "        self.optimizer.step(closure)  # TODO: Does this pass the gradient of loss to the optimizer, which performs GD?\n",
        "        return closure()\n",
        "\n",
        "    def train(self, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            loss = self.step()\n",
        "            print('[%d] loss: %.3f' % (epoch + 1, loss))\n",
        "            self.loss.append(loss.detach().numpy())\n",
        "            self.visualize()\n",
        "\n",
        "    def visualize(self):\n",
        "        data = np.array([self.simulation.state_trajectory[i].detach().numpy() for i in range(self.simulation.T)])\n",
        "        actions = np.array([self.simulation.action_trajectory[i].detach().numpy() for i in range(self.simulation.T)])\n",
        "        fig, axs = plt.subplots(2)\n",
        "        x1 = data[:, 0]\n",
        "        y1 = data[:, 1]\n",
        "        x2 = data[:, 2]\n",
        "        y2 = data[:, 3]\n",
        "        axs[0].plot(x1, y1)\n",
        "        axs[0].grid()\n",
        "        axs[0].set_xlabel('Distance to Landing')\n",
        "        axs[0].set_ylabel('Vertical Velocity')\n",
        "        axs[1].plot(x2, y2)\n",
        "        axs[1].grid()\n",
        "        axs[1].set_xlabel('Orientation')\n",
        "        axs[1].set_ylabel('Angular Velocity')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "U_7s7twlvLnb"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now it's time to run the code!\n",
        "\n",
        "store_loss = []\n",
        "T = 100  # number of time steps\n",
        "dim_input = 4  # state space dimensions\n",
        "dim_hidden = 16  # latent dimensions\n",
        "dim_output = 2  # action space dimensions\n",
        "d = Dynamics()  # define dynamics\n",
        "c = Controller(dim_input, dim_hidden, dim_output)  # define controller\n",
        "s = Simulation(c, d, T)  # define simulation\n",
        "o = Optimize(s)  # define optimizer\n",
        "o.train(50)  # solve the optimization problem\n",
        "store_loss.append(o.loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "q_xrSpHPvSAk",
        "outputId": "5b150faf-aa4a-491c-ab72-f7e365758e25"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-6d07560fe634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# define simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# define optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# solve the optimization problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mstore_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-53d684dbb0ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[%d] loss: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-53d684dbb0ac>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: Does this pass the gradient of loss to the optimizer, which performs GD?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-53d684dbb0ac>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: Why is this necessary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-0150da1da9e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_trajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_trajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-af56aade219d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(state, action)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Note: Same reason as above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Note: Action provided by controller to determine thrust or no thrust.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mdelta_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBOOST_ACCEL\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mFRAME_TIME\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#delta state y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cos' is not defined"
          ]
        }
      ]
    }
  ]
}